{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40736912",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Inicializa cliente (asegúrate de tener tu API key en .env o en variable de entorno)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m():\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Mensaje de sistema: define el rol/persona del bot\u001b[39;00m\n\u001b[32m      8\u001b[39m     mensajes = [\n\u001b[32m      9\u001b[39m         {\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m         }\n\u001b[32m     16\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Usuarios\\Hapema\\Desktop\\Ingenier-a-de-Soluciones-con-Inteligencia-Artificial\\.venv\\Lib\\site-packages\\openai\\_client.py:135\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    133\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m     )\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Inicializa cliente (asegúrate de tener tu API key en .env o en variable de entorno)\n",
    "client = OpenAI()\n",
    "\n",
    "def chat():\n",
    "    # Mensaje de sistema: define el rol/persona del bot\n",
    "    mensajes = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Eres un experto en tecnología que explica conceptos complejos \"\n",
    "                \"de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"Chatbot iniciado 🤖 (escribe 'salir' para terminar)\\n\")\n",
    "\n",
    "    while True:\n",
    "        # Input del usuario\n",
    "        user_input = input(\"Tú: \")\n",
    "        if user_input.lower() == \"salir\":\n",
    "            print(\"Chatbot finalizado 👋\")\n",
    "            break\n",
    "\n",
    "        # Se agrega a la conversación\n",
    "        mensajes.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        try:\n",
    "            # Llamada a la API con todo el historial\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=mensajes,\n",
    "                temperature=0.7,\n",
    "                max_tokens=300\n",
    "            )\n",
    "\n",
    "            # Respuesta del asistente\n",
    "            reply = response.choices[0].message.content\n",
    "            print(\"Bot:\", reply, \"\\n\")\n",
    "\n",
    "            # Guardar respuesta en el historial\n",
    "            mensajes.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar el chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab37f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    mensajes = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres el asistente virtual de EcoMarket.\"}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Tú: \")\n",
    "        if user_input.lower() == \"salir\":\n",
    "            break\n",
    "\n",
    "        mensajes.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=mensajes,\n",
    "                temperature=0.3,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            reply = response.choices[0].message.content\n",
    "            print(\"Bot:\", reply, \"\\n\")\n",
    "\n",
    "            mensajes.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        except Exception as e:\n",
    "            print(\"Error en llamada:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primera llamada básica al modelo\n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hola, ¿cómo estás? Responde en una oración.\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta del Modelo ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n=== Información Técnica ===\")\n",
    "        print(f\"Modelo usado: {response.model}\")\n",
    "        print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"Tokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Tokens de salida: {response.usage.completion_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la llamada: {e}\")\n",
    "        print(\"Verifica tu configuración y conexión a internet\")\n",
    "\n",
    "# Ejecutar la función\n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cccf4a",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n",
    "\n",
    "El rol \"system\" permite establecer el comportamiento y contexto del asistente antes de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1b57r1oeo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con mensaje de sistema\n",
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"¿Qué es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a290e",
   "metadata": {},
   "source": [
    "## Explorando Parámetros de Configuración\n",
    "\n",
    "Los parámetros más importantes al hacer llamadas a LLMs son:\n",
    "\n",
    "- **temperature**: Controla la creatividad (0.0 = determinístico, 1.0 = muy creativo)\n",
    "- **max_tokens**: Límite de tokens en la respuesta\n",
    "- **model**: El modelo específico a usar (gpt-4o, gpt-3.5-turbo, etc.)\n",
    "- **messages**: Array de mensajes con roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j48bg48xqs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando diferentes valores de temperature\n",
    "def comparar_temperature():\n",
    "    prompt = \"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEMPERATURE: {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fc272",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
